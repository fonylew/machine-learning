{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hiive.mdptoolbox import mdp, example\n",
    "from mdp import MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, gym_walk, numpy as np\n",
    "env = gym.make('WalkFive-v0')\n",
    "pi = lambda x: np.random.randint(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'int'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m             state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m V\n\u001b[0;32m---> 16\u001b[0m V \u001b[39m=\u001b[39m td(pi, env)\n\u001b[1;32m     17\u001b[0m V\n",
      "Cell \u001b[0;32mIn [5], line 9\u001b[0m, in \u001b[0;36mtd\u001b[0;34m(pi, env, gamma, alpha, n_episodes)\u001b[0m\n\u001b[1;32m      7\u001b[0m action \u001b[39m=\u001b[39m pi(state)\n\u001b[1;32m      8\u001b[0m \u001b[39m# s_prime, reward, done, info = self.env.step(action)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m next_state, reward, done, info, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     10\u001b[0m td_target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m V[next_state] \u001b[39m*\u001b[39m (\u001b[39mnot\u001b[39;00m done)\n\u001b[1;32m     11\u001b[0m td_error \u001b[39m=\u001b[39m td_target \u001b[39m-\u001b[39m V[state]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "def td(pi, env, gamma=1.0, alpha=0.01, n_episodes=100000):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    for t in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = pi(state)\n",
    "            # s_prime, reward, done, info = self.env.step(action)\n",
    "            next_state, reward, done, info, _ = env.step(action)\n",
    "            td_target = reward + gamma * V[next_state] * (not done)\n",
    "            td_error = td_target - V[state]\n",
    "            V[state] = V[state] + alpha * td_error\n",
    "            state = next_state\n",
    "    return V\n",
    "\n",
    "V = td(pi, env)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(object):\n",
    "    def __init__(self):\n",
    "        self.Q = None\n",
    "        self.alpha = 0.2  # param\n",
    "        self.gamma = 0.90\n",
    "        self.epsilon = 0.25 # param\n",
    "        self.n_episodes = 1_000 # param\n",
    "        self.env = gym.make(\"WalkFive-v0\").env\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"Create the Q table\"\"\"\n",
    "        self.Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        for n in range(self.n_episodes):\n",
    "            # Initialize S\n",
    "            s = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            # Loop for each step of episode\n",
    "            while not done:\n",
    "                # Choose A from S\n",
    "                if np.random.rand() <= self.epsilon:\n",
    "                    a = np.random.randint(self.env.action_space.n)\n",
    "                else:\n",
    "                    a = np.argmax(self.Q[s, :])\n",
    "                \n",
    "                # Take action A\n",
    "                s_prime, reward, done, info = self.env.step(a)\n",
    "                \n",
    "                # Update Q Table\n",
    "                self.Q[s, a] += self.alpha * (reward + self.gamma * self.Q[s_prime, np.argmax(self.Q[s_prime, :])] - self.Q[s, a])\n",
    "                s = s_prime\n",
    "        return reward\n",
    "\n",
    "    def Q_table(self, state, action):\n",
    "        \"\"\"return the optimal value for State-Action pair in the Q Table\"\"\"\n",
    "        return self.Q[state][action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = QLearningAgent()\n",
    "agent.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.          , 0.          ],\n",
       "       [0.          , 0.1292895743],\n",
       "       [0.          , 0.7254857673],\n",
       "       [0.6376480195, 0.81        ],\n",
       "       [0.7207472225, 0.9         ],\n",
       "       [0.8082280821, 1.          ],\n",
       "       [0.          , 0.          ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Walk\n",
      "\n",
      "Value-iteration\n",
      "\n",
      "---> converged at iteration 6 in 0.0008 seconds\n",
      "\n",
      "---> optimal V = \n",
      "[0.     0.6561 0.729  0.81   0.9    1.     0.    ]\n",
      "\n",
      "---> optimal policy = \n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "---> optimal policy actions = \n",
      "[0 1 1 1 1 1 0]\n",
      "\n",
      "Policy-iteration\n",
      "PI iteration 1\n",
      "PI iteration 2\n",
      "\n",
      "---> converged at iteration 2 in 0.0016 seconds\n",
      "\n",
      "---> optimal V = \n",
      "[0.     0.6561 0.729  0.81   0.9    1.     0.    ]\n",
      "\n",
      "---> optimal policy = \n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "---> optimal policy actions = \n",
      "[0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "walk = MDP(environment='WalkFive-v0', convergence_threshold=0.00001, grid=False)\n",
    "print('Random Walk')\n",
    "\n",
    "walk.value_iteration(iterations_to_save=[1, 2, 4, 6, 8, 10, 12], visualize=False)\n",
    "walk.policy_iteration(iterations_to_save=[1, 2, 4, 8, 10], visualize=False)\n",
    "# walk.Q_learning(num_episodes=1000, learning_rate_decay=0.995, epsilon_decay=0.995, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi\n",
      "\n",
      "Value-iteration\n",
      "\n",
      "---> converged at iteration 63 in 0.8382 seconds\n",
      "\n",
      "---> optimal V = \n",
      "[ 89.4734925232  32.8198671487  55.2643920487  37.5775540189\n",
      "   8.4324828228  32.8198671487   8.4323823484  15.2844751006\n",
      "  32.8199676232  18.0940143619  55.2643920487  21.2154953668\n",
      "  12.7561965968  18.0940143619  12.7560961224  37.5775540189\n",
      " 100.5261432709  37.5776918439  62.5160528439  42.864033517\n",
      "  79.5261432709  28.5378804339  48.7379528439  32.819798617\n",
      "  10.4805769371  37.5776918439  10.4804865101  18.0939458301\n",
      "  28.5379708609  15.2846129257  48.7379528439  18.0939458301\n",
      "  15.2847033527  21.2156331918  15.2846129257  42.864033517\n",
      "  89.4735289438  42.8641575595  55.2644475595  48.7378911653\n",
      "  42.8642389438  12.7561516331  24.6840923905  15.2845512471\n",
      "  24.6841737748  70.5734475595  24.6840923905  37.5776301653\n",
      "  24.6841737748  12.7561516331  42.8641575595  15.2845512471\n",
      "  18.0941512569  24.6840923905  18.0940698726  48.7378911653\n",
      "  48.7380760494  79.5261028035  48.7380028035  55.2643920487\n",
      "  37.5778150494  10.4805364698  21.2156831514  12.7560961224\n",
      "  28.5380036394  79.5261028035  28.5379303935  42.8641020487\n",
      "  21.2157563973  10.4805364698  37.5777418035  12.7560961224\n",
      "  21.2157563973  28.5379303935  21.2156831514  55.2643920487\n",
      "  42.8642684445  89.4734925232  42.8642025232  62.5160528439\n",
      "  32.8200335445   8.4324828228  18.0941148363  10.4804865101\n",
      "  32.8200335445  89.4734925232  32.8199676232  48.7379528439\n",
      "  18.0941807576   8.4324828228  32.8199676232  10.4804865101\n",
      "  18.0941807576  24.6841373542  18.0941148363  48.7379528439\n",
      "  37.5778416    100.5261432709  37.5777822709  55.2644475595\n",
      "  79.5261432709  28.5378804339  48.7379528439  32.819798617\n",
      "  10.4805769371  37.5776918439  10.4804865101  18.0939458301\n",
      "  37.5777822709  21.2156331918  62.5160528439  24.683968348\n",
      "  15.2847033527  21.2156331918  15.2846129257  42.864033517\n",
      "  89.4735289438  42.8641575595  70.5734475595  48.7378911653\n",
      "  70.5735289438  24.6840923905  42.8641575595  28.5378187553\n",
      "  12.7562330174  42.8641575595  12.7561516331  21.2155715132\n",
      "  32.8200040438  18.0940698726  55.2644475595  21.2155715132\n",
      "  18.0941512569  24.6840923905  18.0940698726  48.7378911653\n",
      "  79.5261760494  48.7380028035  62.5161028035  55.2643920487\n",
      "  48.7380760494  15.2846628853  28.5379303935  18.0940143619\n",
      "  21.2157563973  62.5161028035  21.2156831514  32.8198671487\n",
      "  28.5380036394  15.2846628853  48.7380028035  18.0940143619\n",
      "  21.2157563973  28.5379303935  21.2156831514  55.2643920487\n",
      "  55.2645584445  70.5734925232  55.2644925232  62.5160528439\n",
      "  42.8642684445  12.7561965968  24.6841373542  15.2846129257\n",
      "  24.6842032755  70.5734925232  24.6841373542  37.5776918439\n",
      "  24.6842032755  12.7561965968  42.8642025232  15.2846129257\n",
      "  24.6842032755  32.8199676232  24.6841373542  62.5160528439\n",
      "  48.7381026     79.5261432709  48.7380432709  70.5734475595\n",
      "  37.5778416     10.4805769371  21.2157236188  12.7561516331\n",
      "  28.53803019    79.5261432709  28.5379708609  42.8641575595\n",
      "  21.2157829479  10.4805769371  37.5777822709  12.7561516331\n",
      "  21.2157829479  28.5379708609  21.2157236188  55.2644475595\n",
      "  42.86429234    89.4735289438  42.8642389438  62.5161028035\n",
      "  70.5735289438  24.6840923905  42.8641575595  28.5378187553\n",
      "  12.7562330174  42.8641575595  12.7561516331  21.2155715132\n",
      "  42.8642389438  24.6840923905  70.5734475595  28.5378187553\n",
      "  18.0941512569  24.6840923905  18.0940698726  48.7378911653\n",
      "  79.5261760494  48.7380028035  79.5261028035  55.2643920487\n",
      "  62.5161760494  21.2156831514  37.5777418035  24.6840368797\n",
      "  15.2847361312  48.7380028035  15.2846628853  24.6840368797\n",
      "  37.5778150494  21.2156831514  62.5161028035  24.6840368797\n",
      "  21.2157563973  28.5379303935  21.2156831514  55.2643920487\n",
      "  70.5735584445  55.2644925232  70.5734925232  62.5160528439\n",
      "  55.2645584445  18.0941148363  32.8199676232  21.2156331918\n",
      "  18.0941807576  55.2644925232  18.0941148363  28.5378804339\n",
      "  32.8200335445  18.0941148363  55.2644925232  21.2156331918\n",
      "  24.6842032755  32.8199676232  24.6841373542  62.5160528439\n",
      "  62.5162026     62.5161432709  62.5161432709  70.5734475595\n",
      "  48.7381026     15.2847033527  28.5379708609  18.0940698726\n",
      "  21.2157829479  62.5161432709  21.2157236188  32.8199226595\n",
      "  28.53803019    15.2847033527  48.7380432709  18.0940698726\n",
      "  28.53803019    37.5777822709  28.5379708609  70.5734475595\n",
      "  55.26458234    70.5735289438  55.2645289438  79.5261028035\n",
      "  42.86429234    12.7562330174  24.6841737748  15.2846628853\n",
      "  24.684227171   70.5735289438  24.6841737748  37.5777418035\n",
      "  24.684227171   12.7562330174  42.8642389438  15.2846628853\n",
      "  24.684227171   32.8200040438  24.6841737748  62.5161028035\n",
      "  48.738124106   79.5261760494  48.7380760494  70.5734925232\n",
      "  62.5161760494  21.2156831514  37.5777418035  24.6840368797\n",
      "  10.4806097157  37.5777418035  10.4805364698  18.0940143619\n",
      "  48.7380760494  28.5379303935  79.5261028035  32.8198671487\n",
      "  15.2847361312  21.2156831514  15.2846628853  42.8641020487\n",
      "  70.5735584445  42.8642025232  89.4734925232  48.7379528439\n",
      "  55.2645584445  18.0941148363  32.8199676232  21.2156331918\n",
      "  12.7562625181  42.8642025232  12.7561965968  21.2156331918\n",
      "  32.8200335445  18.0941148363  55.2644925232  21.2156331918\n",
      "  18.0941807576  24.6841373542  18.0941148363  48.7379528439\n",
      "  62.5162026     48.7380432709  62.5161432709  55.2644475595\n",
      "  48.7381026     15.2847033527  28.5379708609  18.0940698726\n",
      "  15.2847626818  48.7380432709  15.2847033527  24.6840923905\n",
      "  28.53803019    15.2847033527  48.7380432709  18.0940698726\n",
      "  21.2157829479  28.5379708609  21.2157236188  55.2644475595\n",
      "  55.26458234    55.2645289438  55.2645289438  62.5161028035\n",
      "  42.86429234    12.7562330174  24.6841737748  15.2846628853\n",
      "  18.0942046531  55.2645289438  18.0941512569  28.5379303935\n",
      "  24.684227171   12.7562330174  42.8642389438  15.2846628853\n",
      "  32.82005744    42.8642389438  32.8200040438  79.5261028035\n",
      "  48.738124106   62.5161760494  48.7380760494  89.4734925232\n",
      "  37.577863106   10.4806097157  21.2157563973  12.7561965968\n",
      "  21.2158044539  62.5161760494  21.2157563973  32.8199676232\n",
      "  21.2158044539  10.4806097157  37.5778150494  12.7561965968\n",
      "  28.538051696   37.5778150494  28.5380036394  70.5734925232\n",
      "  42.8643116954  70.5735584445  42.8642684445  79.5261432709\n",
      "  55.2645584445  18.0941148363  32.8199676232  21.2156331918\n",
      "   8.4325487441  32.8199676232   8.4324828228  15.2846129257\n",
      "  55.2645584445  32.8199676232  89.4734925232  37.5776918439\n",
      "  12.7562625181  18.0941148363  12.7561965968  37.5776918439\n",
      "  62.5162026     37.5777822709 100.5261432709  42.8641575595\n",
      "  48.7381026     15.2847033527  28.5379708609  18.0940698726\n",
      "  10.4806362663  37.5777822709  10.4805769371  18.0940698726\n",
      "  28.53803019    15.2847033527  48.7380432709  18.0940698726\n",
      "  15.2847626818  21.2157236188  15.2847033527  42.8641575595\n",
      "  55.26458234    42.8642389438  55.2645289438  48.7380028035\n",
      "  42.86429234    12.7562330174  24.6841737748  15.2846628853\n",
      "  12.7562864136  42.8642389438  12.7562330174  21.2156831514\n",
      "  24.684227171   12.7562330174  42.8642389438  15.2846628853\n",
      "  18.0942046531  24.6841737748  18.0941512569  48.7380028035\n",
      "  48.738124106   48.7380760494  48.7380760494  55.2644925232\n",
      "  37.577863106   10.4806097157  21.2157563973  12.7561965968\n",
      "  15.2847841878  48.7380760494  15.2847361312  24.6841373542\n",
      "  21.2158044539  10.4806097157  37.5778150494  12.7561965968\n",
      "  37.577863106   48.7380760494  37.5778150494  89.4734925232\n",
      "  42.8643116954  55.2645584445  42.8642684445 100.5261432709\n",
      "  32.8200767954   8.4325487441  18.0941807576  10.4805769371\n",
      "  18.0942240085  55.2645584445  18.0941807576  28.5379708609\n",
      "  18.0942240085   8.4325487441  32.8200335445  10.4805769371\n",
      "  32.8200767954  42.8642684445  32.8200335445  79.5261432709\n",
      "  37.5778805259  62.5162026     37.5778416     89.4735289438]\n",
      "\n",
      "---> optimal policy = \n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "---> optimal policy actions = \n",
      "[4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
      " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 2 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
      " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 2 1 3 3 3 3 2 2 2 2 3 2 3\n",
      " 2 3 3 3 3 1 2 2 2 3 3 3 3 0 0 0 0 3 2 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
      " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 2 1 1 1 1 1\n",
      " 2 2 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1\n",
      " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TaxiEnv' object has no attribute 'nrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m taxi \u001b[39m=\u001b[39m MDP(environment\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTaxi-v3\u001b[39m\u001b[39m'\u001b[39m, convergence_threshold\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, grid\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTaxi\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m taxi\u001b[39m.\u001b[39;49mvalue_iteration(iterations_to_save\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m10\u001b[39;49m, \u001b[39m15\u001b[39;49m, \u001b[39m20\u001b[39;49m, \u001b[39m25\u001b[39;49m, \u001b[39m30\u001b[39;49m, \u001b[39m35\u001b[39;49m, \u001b[39m40\u001b[39;49m, \u001b[39m45\u001b[39;49m, \u001b[39m50\u001b[39;49m], visualize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m taxi\u001b[39m.\u001b[39mpolicy_iteration(iterations_to_save\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m16\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m], visualize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m taxi\u001b[39m.\u001b[39mQ_learning(num_episodes\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, learning_rate_decay\u001b[39m=\u001b[39m\u001b[39m0.995\u001b[39m, epsilon_decay\u001b[39m=\u001b[39m\u001b[39m0.995\u001b[39m, visualize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/machine-learning/reinforcement_learning/mdp.py:393\u001b[0m, in \u001b[0;36mMDP.value_iteration\u001b[0;34m(self, iterations_to_save, visualize)\u001b[0m\n\u001b[1;32m    390\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid:\n\u001b[0;32m--> 393\u001b[0m     utils\u001b[39m.\u001b[39mplot_heatmap_value_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimal_V, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mnrow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mncol)\n\u001b[1;32m    394\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     utils\u001b[39m.\u001b[39mplot_value_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimal_V, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TaxiEnv' object has no attribute 'nrow'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "taxi = MDP(environment='Taxi-v3', convergence_threshold=0.0001, grid=True)\n",
    "print('Taxi')\n",
    "\n",
    "taxi.value_iteration(iterations_to_save=[1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50], visualize=True)\n",
    "taxi.policy_iteration(iterations_to_save=[0, 1, 8, 16, 32, 64, 128], visualize=True)\n",
    "taxi.Q_learning(num_episodes=10000, learning_rate_decay=0.995, epsilon_decay=0.995, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<WalkEnv<WalkFive-v0>>>>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
